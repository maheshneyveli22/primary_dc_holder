
--GET-STARTED-AI-FUNDAMENTALS--START-

1)Introduction to AI https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/1-introduction

a) AI enables us to build amazing software that can improve health care, enable people to overcome physical disadvantages, empower smart infrastructure, create incredible entertainment experiences, and even save the planet!


b)
b.1)  Machine learning - This is often the foundation for an AI system, and is the way we "teach" a computer model to make predictions and draw conclusions from data.
b.2) Computer vision - Capabilities within AI to interpret the world visually through cameras, video, and images.
b.3) Natural language processing - Capabilities within AI for a computer to interpret written or spoken language, and respond in kind.
b.4) Document intelligence - Capabilities within AI that deal with managing, processing, and using high volumes of data found in forms and documents.
b.5) Knowledge mining - Capabilities within AI to extract information from large volumes of often unstructured data to create a searchable knowledge store.
b.6) Generative AI - Capabilities within AI that create original content in a variety of formats including natural language, image, code, and more.


2) Understand machine learning 

https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/2-understand-machine-learn

-> Machine Learning is the foundation for most AI solutions. Since the 1950's, researchers, often known as data scientists, have worked on different approaches to AI. 
-> Most modern applications of AI have their origins in machine learning, a branch of AI that combines computer science and mathematics.

-> Let's start by looking at a real-world example of how machine learning can be used to solve a difficult problem.

-> Sustainable farming techniques are essential to maximize food production while protecting a fragile environment. 
-> The Yield, an agricultural technology company based in Australia, uses sensors, data, and machine learning to help farmers make informed decisions related to weather, soil, and plant conditions.


a) How machine learning works
-> So how do machines learn?

-> The answer is, from data. In today's world, we create huge volumes of data as we go about our everyday lives. 
-> From the text messages, emails, and social media posts we send to the photographs and videos we take on our phones, we generate massive amounts of information.
->  More data still is created by millions of sensors in our homes, cars, cities, public transport infrastructure, and factories.

-> Data scientists can use all of that data to train machine learning models that can make predictions and inferences based on the relationships they find in the data.

-> Machine learning models try to capture the relationship between data.
-> For example, suppose an environmental conservation organization wants volunteers to identify and catalog different species of wildflower using a phone app. The following animation shows how machine learning can be used to enable this scenario.

b) Sample Scenario:
a) A team of botanists and scientists collect data on wildflower samples.
b) The team labels the samples with the correct species.
c) The labeled data is processed using an algorithm that finds relationships between the features of the samples and the labeled species.
d) The results of the algorithm are encapsulated in a model.
e) When new samples are found by volunteers, the model can identify the correct species label.
f) Approaches to AI have advanced to complete tasks of much greater complexity. 
g) These complex models form the basis of AI capabilities.


c) Machine learning in Microsoft Azure
-> Microsoft Azure provides the Azure Machine Learning service - a cloud-based platform for creating, managing, and publishing machine learning models. 
-> Azure Machine Learning Studio offers multiple authoring experiences such as:

c.1) Automated machine learning: this feature enables non-experts to quickly create an effective machine learning model from data.
c.2) Azure Machine Learning designer: a graphical interface enabling no-code development of machine learning solutions.
c.3) Data metric visualization: analyze and optimize your experiments with visualization.
c.4) Notebooks: write and run your own code in managed Jupyter Notebook servers that are directly integrated in the studio.


3) Understand computer vision


https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/3-understand-computer-vision

-> Computer Vision is an area of AI that deals with visual processing. Let's explore some of the possibilities that computer vision brings.
-> The Seeing AI app is a great example of the power of computer vision. 
-> Designed for the blind and low vision community, the Seeing AI app harnesses the power of AI to open up the visual world and describe nearby people, text and objects.

a) Computer Vision models and capabilities
a.1) Image classification	

a.1.1) Image classification involves training a machine learning model to classify images based on their contents.
a.1.2) For example, in a traffic monitoring solution you might use an image classification model to classify images based on the type of vehicle they contain, such as taxis, buses, cyclists, and so on.

a.2) Object detection	
-> Object detection machine learning models are trained to classify individual objects within an image, and identify their location with a bounding box. 
-> For example, a traffic monitoring solution might use object detection to identify the location of different classes of vehicle.


a.3) Semantic segmentation	
-> Semantic segmentation is an advanced machine learning technique in which individual pixels in the image are classified according to the object to which they belong. 
-> For example, a traffic monitoring solution might overlay traffic images with "mask" layers to highlight different vehicles using specific colors.

a.4) Image analysis:
-> You can create solutions that combine machine learning models with advanced image analysis techniques to extract information from images, including "tags" that could help catalog the image or even descriptive captions that summarize the scene shown in the image.
a.5) Face detection, analysis, and recognition	
-> Face detection is a specialized form of object detection that locates human faces in an image. -> This can be combined with classification and facial geometry analysis techniques to recognize individuals based on their facial features.
a.6) Optical character recognition (OCR)	
-> Optical character recognition is a technique used to detect and read text in images. 
-> You can use OCR to read text in photographs (for example, road signs or store fronts) or to extract information from scanned documents such as letters, invoices, or forms.


b) Computer vision services in Microsoft Azure
-> You can use Microsoft's Azure AI Vision to develop computer vision solutions. 
-> The service features are available for use and testing in the Azure Vision Studio and other programming languages. 

Some features of Azure AI Vision include:
b.1) Image Analysis: capabilities for analyzing images and video, and extracting descriptions, tags, objects, and text.
b.2) Face: capabilities that enable you to build face detection and facial recognition solutions.
b.3)Optical Character Recognition (OCR): capabilities for extracting printed or handwritten text from images, enabling access to a digital version of the scanned text.


4) Understand natural language processing

https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/4-understand-natural-language-process

-> Natural language processing (NLP) is the area of AI that deals with creating software that understands written and spoken language.

a) NLP enables you to create software that can:

-> Analyze and interpret text in documents, email messages, and other sources.
-> Interpret spoken language, and synthesize speech responses.
-> Automatically translate spoken or written phrases between languages.
-> Interpret commands and determine appropriate actions.

b) For example, Starship Commander is a virtual reality (VR) game from Human Interact that takes place in a science fiction world. The game uses natural language processing to enable players to control the narrative and interact with in-game characters and starship systems.

c) Natural language processing in Microsoft Azure
c.1)  You can use Microsoft's Azure AI Language to build natural language processing solutions. 
c.2)  Some features of Azure AI Language include understanding and analyzing text, training conversational language models that can understand spoken or text-based commands, and building intelligent applications.

c.3)  Microsoft's Azure AI Speech is another service that can be used to build natural language processing solutions. Azure AI Speech features include speech recognition and synthesis, real-time translations, conversation transcriptions, and more.

c.4) You can explore Azure AI Language features in the Azure Language Studio and Azure AI Speech features in the Azure Speech Studio. The service features are available for use and testing in the studios and other programming languages.


5) Understand document intelligence and knowledge mining

https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/5-document-intelligence-knowledge-mining

-> Document Intelligence is the area of AI that deals with managing, processing, and using high volumes of a variety of data found in forms and documents. 
-> Document intelligence enables you to create software that can automate processing for contracts, health documents, financial forms and more


DOCUMENT INTELLIGENCE IN MICROSOFT AZURE
a) You can use Microsoft's Azure AI Document Intelligence to build solutions that manage and accelerate data collection from scanned documents. 
b) Features of Azure AI Document Intelligence help automate document processing in applications and workflows, enhance data-driven strategies, and enrich document search capabilities. 
-> You can use prebuilt models to add intelligent document processing for invoices, receipts, health insurance cards, tax forms, and more. 
-> You can also use Azure AI Document Intelligence to create custom models with your own labeled datasets. The service features are available for use and testing in the Document Intelligence Studio and other programming languages.



c) This example shows information extracted from a tax form using Azure AI Document Intelligence.

6) Knowledge Mining
-> Knowledge mining is the term used to describe solutions that involve extracting information from large volumes of often unstructured data to create a searchable knowledge store.

Knowledge mining in Microsoft Azure
a) One Microsoft knowledge mining solution is AZURE AI SEARCH, an enterprise, search solution that has tools for building indexes. The indexes can then be used for internal only use, or to enable searchable content on public facing internet assets.

b) Azure AI Search can utilize the built-in AI capabilities of Azure AI services such as image processing, document intelligence, and natural language processing to extract data. 
c) The product's AI capabilities makes it possible to index previously unsearchable documents and to extract and surface insights from large amounts of data quickly.

d) Screenshot of a search web page for a travel company.

e) In this example, a travel web site uses Azure AI Search to power searching for information about destinations based on information extracted from images or text using AI services.




7) Understand generative AI:
-> Generative AI describes a category of capabilities within AI that create original content. 
-> People typically interact with generative AI that has been built into chat applications. 
-> Generative AI applications take in natural language input, and return appropriate responses in a variety of formats including natural language, image, code, and audio.


a) Generative AI in Microsoft Azure
-> Azure OpenAI Service is Microsoft's cloud solution for deploying, customizing, and hosting generative AI models. 
-> It brings together the best of OpenAI's cutting edge models and APIs with the security and scalability of the Azure cloud platform.
-> Azure OpenAI Service supports many generative model choices that can serve different needs. 

b) You can use AZURE AI STUDIO to create generative AI solutions, such as custom copilot chat-based assistants that use Azure OpenAI Service models
c) In this example, an Azure OpenAI Service model is used to power a copilot application that can be used to generate original content in response to user prompts, such as a request to write a cover letter.


8) Challenges and risks with AI

-> Artificial Intelligence is a powerful tool that can be used to greatly benefit the world. However, like any tool, it must be used responsibly.

The following table shows some of the potential challenges and risks facing an AI application developer.

a) Challenge or Risk--->Example
a.1) Bias can affect results	A loan-approval model discriminates by gender due to bias in the data with which it was trained
a.2) Errors may cause harm	An autonomous vehicle experiences a system failure and causes a collision
a.3) Data could be exposed	A medical diagnostic bot is trained using sensitive patient data, which is stored insecurely
a.4) Solutions may not work for everyone	A home automation assistant provides no audio output for visually impaired users
a.5) Users must trust a complex system	An AI-based financial tool makes investment recommendations - what are they based on?
a.6) Who's liable for AI-driven decisions?	An innocent person is convicted of a crime based on evidence from facial recognition – who's responsible?


9) Understand Responsible AI

https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/8a-understand-responsible-ai

-> At Microsoft, AI software development is guided by a set of SIX principles, designed to ensure that AI applications provide amazing solutions to difficult problems without any unintended negative consequences.


a) Fairness: 
-> AI systems should treat all people fairly. For example, suppose you create a machine learning model to support a loan approval application for a bank. The model should predict whether the loan should be approved or denied without bias. This bias could be based on gender, ethnicity, or other factors that result in an unfair advantage or disadvantage to specific groups of applicants.

-> Azure Machine Learning includes the capability to interpret models and quantify the extent to which each feature of the data influences the model's prediction. This capability helps data scientists and developers identify and mitigate bias in the model.

-> Another example is Microsoft's implementation of Responsible AI with the Face service, which retires facial recognition capabilities that can be used to try to infer emotional states and identity attributes. These capabilities, if misused, can subject people to stereotyping, discrimination or unfair denial of services.

For more details about considerations for fairness, watch the following video.

b) Reliability and safety
-> AI systems should perform reliably and safely. For example, consider an AI-based software system for an autonomous vehicle; or a machine learning model that diagnoses patient symptoms and recommends prescriptions.
-> Unreliability in these kinds of systems can result in substantial risk to human life.

-> AI-based software application development must be subjected to rigorous testing and deployment management processes to ensure that they work as expected before release.

c) Privacy and security
-> AI systems should be secure and respect privacy. 
-> The machine learning models on which AI systems are based rely on large volumes of data, which may contain personal details that must be kept private. 
-> Even after the models are trained and the system is in production, privacy and security need to be considered. 
-> As the system uses new data to make predictions or take action, both the data and decisions made from the data may be subject to privacy or security concerns.

d) Inclusiveness
-> AI systems should empower everyone and engage people. 
-> AI should bring benefits to all parts of society, regardless of physical ability, gender, sexual orientation, ethnicity, or other factors.

e) Transparency
-> AI systems should be understandable. 
-> Users should be made fully aware of the purpose of the system, how it works, and what limitations may be expected.

f) Accountability
-> People should be accountable for AI systems. 
-> Designers and developers of AI-based solutions should work within a framework of governance and organizational principles that ensure the solution meets ethical and legal standards that are clearly defined.

--> The principles of responsible AI can help you understand some of the challenges facing developers as they try to create ethical AI solutions.

https://www.microsoft.com/ai/responsible-ai-resources

10) Summary: 
-> Artificial Intelligence enables the creation of powerful solutions to many kinds of problems. 
-> AI systems can exhibit human characteristics to analyze the world around them, make predictions or inferences, and act on them in ways that we could only imagine a short time ago.

-> With this power, comes responsibility. As developers of AI solutions, we must apply principles that ensure that everyone benefits from AI without disadvantaging any individual or section of society.


...get-started-ai-fundamentals... Done ... 

https://learn.microsoft.com/en-us/training/modules/get-started-ai-fundamentals/9-summary#completion

--GET-STARTED-AI-FUNDAMENTALS--END-


---FUNDAMENTALS OF MACHINE LEARNING---

11) Fundamentals of machine learning - Introduction

https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/1-introduction

a) Machine learning is in many ways the intersection of two disciplines - data science and software engineering.
b) The goal of machine learning is to use data to create a predictive model that can be incorporated into a software application or service.
c) To achieve this goal requires collaboration between data scientists who explore and prepare the data before using it to train a machine learning model, and software developers who integrate the models into applications where they're used to predict new data values (a process known as inferencing).

-> In this module, you'll explore some of the core concepts on which machine learning is based, learn how to identify different kinds of machine learning models, and examine the ways in which machine learning models are trained and evaluated
-> Finally, you'll learn how to use Microsoft Azure Machine Learning to train and deploy a machine learning model, without needing to write any code.

d) Note:
Machine learning is based on mathematical and statistical techniques, some of which are described at a high level in this module. Don't worry if you're not a mathematical expert though! The goal of the module is to help you gain an intuition of how machine learning works - we'll keep the mathematics to the minimum required to understand the core concepts.

12) What is machine learning?


https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/2-what-is-machine-learning

a) Machine learning has its origins in statistics and mathematical modeling of data. 
-> The fundamental idea of machine learning is to use data from past observations to predict unknown outcomes or values. For example:

Examples:
● The proprietor of an ice cream store might use an app that combines historical sales and weather records to predict how many ice creams they're likely to sell on a given day, based on the weather forecast.
● A doctor might use clinical data from past patients to run automated tests that predict whether a new patient is at risk from diabetes based on factors like weight, blood glucose level, and other measurements.
● A researcher in the Antarctic might use past observations automate the identification of different penguin species (such as Adelie, Gentoo, or Chinstrap) based on measurements of a bird's flippers, bill, and other physical attributes.


13) Machine learning as a function

https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/2-what-is-machine-learning

-> Because machine learning is based on mathematics and statistics, it's common to think about machine learning models in mathematical terms. 
-> Fundamentally, a machine learning model is a software application that encapsulates a function to calculate an output value based on one or more input values. 
a) What is TRAINING: 
The process of defining that function is known as training. 

b) What is INFERENCING:
After the function has been defined, you can use it to predict new values in a process called inferencing.

c) Steps involved in training and inferencing: 

Arrive At Vector/Label(1) using features(x1,x2..xn) ----> Develop algorithm to calculate label based on Vector(2)---> Result of algorithm is a model which encapsulates function y=f(x)---> Use Trained Model for inferencing 


c.1) The training data consists of past observations.
-> In most cases, the observations include the observed attributes or features of the thing being observed, and the known value of the thing you want to train a model to predict (known as the LABEL).

-> In mathematical terms, you'll often see the FEATURES referred to using the shorthand variable name x, and the label referred to as y.
-> Usually, an observation consists of multiple feature values, so x is actually a vector (an array with multiple values), like this: [x1,x2,x3,...].

c.1.1) Example scenario:  
To make this clearer, let's consider the examples described previously:

ICE CREAM: 
-> In the ice cream sales scenario, our goal is to train a model that can PREDICT THE NUMBER OF ICE CREAM SALES based on the weather. The weather measurements for the day (temperature, rainfall, windspeed, and so on) would be the features (x), and the number of ice creams sold on each day would be the label (y).
DIABETES: 
-> In the medical scenario, the goal is to predict WHETHER OR NOT A PATIENT IS AT RISK OF DIABETES based on their clinical measurements. The patient's measurements (weight, blood glucose level, and so on) are the features (x), and the likelihood of diabetes (for example, 1 for at risk, 0 for not at risk) is the label (y).
PENGUINS-ANTARCTIC: 
In the Antarctic research scenario, we want to predict the species of a penguin based on its physical attributes. The key measurements of the penguin (length of its flippers, width of its bill, and so on) are the features (x), and the species (for example, 0 for Adelie, 1 for Gentoo, or 2 for Chinstrap) is the label (y).

c.2) ALGORITHM: 
-> An algorithm is applied to the data to try to determine a relationship between the features and the label, and generalize that relationship as a calculation that can be performed on x to calculate y. 
-> The specific algorithm used depends on the kind of predictive problem you're trying to solve (more about this later), but the basic principle is to try to fit a function to the data, in which the values of the features can be used to calculate the label.

c.3) MODEL: 
The result of the algorithm is a model that encapsulates the calculation derived by the algorithm as a function - let's call it f. In mathematical notation:

y = f(x)

c.4) INFERENCING: 
Now that the training phase is complete, the trained model can be used for inferencing. The model is essentially a software program that encapsulates the function produced by the training process. You can input a set of feature values, and receive as an output a prediction of the corresponding label. Because the output from the model is a prediction that was calculated by the function, and not an observed value, you'll often see the output from the function shown as ŷ (which is rather delightfully verbalized as "y-hat").


14) Types of machine learning:
 https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/3-types-of-machine-learning
 
 
 -> There are multiple types of machine learning, and you must apply the appropriate type depending on what you're trying to predict. 
 -> A breakdown of common types of machine learning is shown in the following diagram.
 
 ##########
 
 Machine Learning--> Supervised machine learning[x1,x2,x3],y/Unsupervised machine learning [x1,x2,x2]
 
 ...Supervised machine ....
 Supervised machine learning -> Regression/Classification  
 
 Regression: eg: Icecream sales based on weather 
 
 ...classification.. 
 Classification--> Binary Classification/Multiclass classification
 
 Binary Classification: Eg Diabetes 
 Multiclass Classification: Penguin 

..Unsupervised machine learning..
Unsupervised machine learning ---> Clustering 

Clustering: 
Group similar flowers based on their size, number of leaves, and number of petals.


Machine Learning 
      |
	  |
	  |
 ------------------------
 |                       |
 |						 |
 Supervised           Unsupervised---->Clustering
 |
 |       ------> Regression
 |       |
 ---------
         |
		 -------->Classification
		            |
					|
			-----------------
			|               |
			Binary        Multiclass 

##########

a) Supervised machine learning
-> Supervised machine learning is a general term for machine learning algorithms 
-> In which the TRAINING DATA INCLUDES BOTH FEATURE VALUES AND KNOWN LABEL VALUES. 
-> Supervised machine learning is used to train models by determining a relationship between the features and labels in past observations, so that unknown labels can be predicted for features in future cases

a.1) REGRESSION
-> Regression is a form of supervised machine learning in which the label predicted by the model is a NUMERIC VALUE. For example:

-> The number of ice creams sold on a given day, based on the temperature, rainfall, and windspeed.
-> The selling price of a property based on its size in square feet, the number of bedrooms it contains, and socio-economic metrics for its location.
-> The fuel efficiency (in miles-per-gallon) of a car based on its engine size, weight, width, height, and length.

a.2) Classification
-> Classification is a form of supervised machine learning in which the LABEL REPRESENTS A CATEGORIZATION, OR CLASS. There are two common classification scenarios


a.2.1) Binary classification
-> Here Model predicts True or False as output
-> In binary classification, the label determines -->whether the observed item is (or isn't) an instance of a specific class. Or put another way, binary classification models predict one of two mutually exclusive outcomes. For example:

Examples: 
-> Whether a patient is at risk for diabetes based on clinical metrics like weight, age, blood glucose level, and so on.
-> Whether a bank customer will default on a loan based on income, credit history, age, and other factors.
-> Whether a mailing list customer will respond positively to a marketing offer based on demographic attributes and past purchases.

In all of these examples, the model predicts a binary true/false or positive/negative prediction for a single possible class.

a.2.2) Multiclass classification

-> Multiclass classification extends binary classification to predict a label that represents one of multiple possible classes. 


FOR EXAMPLE:
-> The species of a penguin (Adelie, Gentoo, or Chinstrap) based on its physical measurements.
The genre of a movie (comedy, horror, romance, adventure, or science fiction) based on its cast, director, and budget.
-> In most scenarios that involve a known set of multiple classes, multiclass classification is used to predict mutually exclusive labels. 
-> For example, a penguin can't be both a Gentoo and an Adelie. 
-> However, there are also some algorithms that you can use to train multilabel classification models, in which there may be more than one valid label for a single observation. 
-> For example, a movie could potentially be categorized as both science fiction and comedy.


b) Unsupervised machine learning:


-> Unsupervised machine learning involves training models using data that consists only of feature values WITHOUT ANY KNOWN LABELS. 
-> Unsupervised machine learning algorithms determine relationships between the features of the observations in the training data.


EXAMPLE:
-> Group similar flowers based on their size, number of leaves, and number of petals.
-> Identify groups of similar customers based on demographic attributes and purchasing behavior.



-> In some ways, clustering is similar to multiclass classification; in that it categorizes observations into discrete groups. 
-> The difference is that when using classification, you already know the classes to which the observations in the training data belong; 
-> so the algorithm works by determining the relationship between the features and the known classification label. 
-> In clustering, there's no previously known cluster label and the algorithm groups the data observations based purely on similarity of features.

-> In some cases, clustering is used to determine the set of classes that exist before training a classification model. 
-> For example, you might use clustering to segment your customers into groups, and then analyze those groups to identify and categorize different classes of customer (high value - low volume, frequent small purchaser, and so on). 
-> You could then use your categorizations to label the observations in your clustering results and use the labeled data to train a classification model that predicts to which customer category a new customer might belong.


https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/4-regression

15) Regression: 
-> Regression models are trained to predict numeric label values based on training data that includes both features and known labels. 
-> The process for training a regression model (or indeed, any supervised machine learning model) involves multiple iterations:
i) in which you use an appropriate algorithm (usually with some parameterized settings) to train a model, ii) evaluate the model's predictive performance, and iii)refine the model by repeating the training process with different algorithms and parameters until you achieve an acceptable level of predictive accuracy.

a) 4 keys elements of training process for supervised machine learning models:

1) Split the training data (randomly) to create a dataset with which to train the model while holding back a subset of the data that you'll use to validate the trained model.
2) Use an algorithm to fit the training data to a model. In the case of a regression model, use a regression algorithm such as linear regression.
3) Use the validation data you held back to test the model by predicting labels for the features.
4) Compare the known actual labels in the validation dataset to the labels that the model predicted. Then aggregate the differences between the predicted and actual label values to calculate a metric that indicates how accurately the model predicted for the validation data.
5) After each train, validate, and evaluate iteration, you can repeat the process with different algorithms and parameters until an acceptable evaluation metric is achieved.

Split the training data1 ----------------------|
      |										   |	   
	  |                                        |
	  |                                        |
Use algorithm to fit training data2            |
to a model                                     |
	  |                                        |
	  |                                        |
      ↓                                        ↕
Predict futures by using validation data3      |
to test the model                              |
	  |                                        |
	  |                                        ↕
      ↓                                        |
1) Compare know actual labels to labels that   |
model predicted.4                              |
2) Aggegate differences between predicted and  |
actual label values to calculate a metric that |
predicts accuracy of model4                    |
      |										   |
	  |                                        |
	  ------------------------------------------   
	  
	  
b) Example - regression

-> Let's explore regression with a simplified example in which we'll train a model to predict a numeric label (y) based on a single feature value (x). Most real scenarios involve multiple feature values, which adds some complexity; but the principle is the same.
	  
	  
b.1) For our example, let's stick with the ice cream sales scenario we discussed previously. For our feature, we'll consider the temperature (let's assume the value is the maximum temperature on a given day), and the label we want to train a model to predict is the number of ice creams sold that day. We'll start with some historic data that includes records of daily temperatures (x) and ice cream sales (y):

Temperature (x)	Ice cream sales (y)
51					1
52					0
67					14
65					14
70					23
69					20
72					23
75					26
73					22
81					30
78					26
83					36



b.2) Training a regression model
We'll start by splitting the data and using a subset of it to train a model. Here's the training dataset:


Temperature (x)	Ice cream sales (y)
51					1
65					14
69					20
72					23
75					26
81					30
	  
	  
	  
b.3) To get an insight of how these x and y values might relate to one another, we can plot them as coordinates along two axes


b.4) Now we're ready to apply an algorithm to our training data and fit it to a function that applies an operation to x to calculate y. One such algorithm is linear regression, which works by deriving a function that produces a straight line through the intersections of the x and y values while minimizing the average distance between the line and the plotted points, like this:

---> The line is a visual representation of the function in which the slope of the line describes how to calculate the value of y for a given value of x. The line intercepts the x axis at 50, so when x is 50, y is 0. As you can see from the axis markers in the plot, the line slopes so that every increase of 5 along the x axis results in an increase of 5 up the y axis; so when x is 55, y is 5; when x is 60, y is 10, and so on. To calculate a value of y for a given value of x, the function simply subtracts 50; in other words, the function can be expressed like this:

f(x) = x-50

You can use this function to predict the number of ice creams sold on a day with any given temperature. For example, suppose the weather forecast tells us that tomorrow it will be 77 degrees. We can apply our model to calculate 77-50 and predict that we'll sell 27 ice creams tomorrow.

But just how accurate is our model?


b.4) Evaluating a regression model

-> To validate the model and evaluate how well it predicts, we held back some data for which we know the label (y) value. Here's the data we held back:


ACTUAL SALES 
Temperature (x)	Ice cream sales (y)
52						0
67						14
70						23
73						22
78						26
83						36
	  
	  
	  
b.5) We can use the model to predict the label for each of the observations in this dataset based on the feature (x) value; and then compare the predicted label (ŷ) to the known actual label value (y).

Using the model we trained earlier, which encapsulates the function f(x) = x-50, results in the following predictions:



Temperature (x)	Actual sales (y)	Predicted sales (ŷ)
52	                0	                   2
67	                14	                   17
70	                23	                   20
73	                22	                   23
78	                26	                   28
83	                36	                   33

-> We can plot both the predicted and actual labels against the feature values like this:


b.6) The predicted labels are calculated by the model so they're on the function line, but there's some variance between the ŷ values calculated by the function and the actual y values from the validation dataset; which is indicated on the plot as a line between the ŷ and y values that shows how far off the prediction was from the actual value.


c) REGRESSION EVALUATION METRICS
Based on the differences between the predicted and actual values, you can calculate some common metrics that are used to evaluate a regression model.


c.1) Mean Absolute Error (MAE)
-> The variance in this example indicates by how many ice creams each prediction was wrong. 
--> It doesn't matter if the prediction was over or under the actual value (so for example, -3 and +3 both indicate a variance of 3). 
-> This metric is known as the absolute error for each prediction, and can be summarized for the whole validation set as the mean absolute error (MAE).

-> In the ice cream example, the mean (average) of the absolute errors (2, 3, 3, 1, 2, and 3) is 2.33.

c.2) Mean Squared Error (MSE)
The mean absolute error metric takes all discrepancies between predicted and actual labels into account equally. However, it may be more desirable to have a model that is consistently wrong by a small amount than one that makes fewer, but larger errors. One way to produce a metric that "amplifies" larger errors by squaring the individual errors and calculating the mean of the squared values. This metric is known as the mean squared error (MSE).

In our ice cream example, the mean of the squared absolute values (which are 4, 9, 9, 1, 4, and 9) is 6.



c.3) Root Mean Squared Error (RMSE)
-> The mean squared error helps take the magnitude of errors into account, but because it squares the error values, the resulting metric no longer represents the quantity measured by the label. 
->In other words, we can say that the MSE of our model is 6, but that doesn't measure its accuracy in terms of the number of ice creams that were mispredicted; 6 is just a numeric score that indicates the level of error in the validation predictions.

If we want to measure the error in terms of the number of ice creams, we need to calculate the square root of the MSE; which produces a metric called, unsurprisingly, Root Mean Squared Error. In this case √6, which is 2.45 (ice creams).


c.4) Coefficient of determination (R2)
-> All of the metrics so far compare the discrepancy between the predicted and actual values in order to evaluate the model.
->  However, in reality, there's some natural random variance in the daily sales of ice cream that the model takes into account. 
-> In a linear regression model, the training algorithm fits a straight line that minimizes the mean variance between the function and the known label values. 
-> The coefficient of determination (more commonly referred to as R2 or R-Squared) is a metric that measures the proportion of variance in the validation results that can be explained by the model, as opposed to some anomalous aspect of the validation data (for example, a day with a highly unusual number of ice creams sales because of a local festival).

-> The calculation for R2 is more complex than for the previous metrics. It compares the sum of squared differences between predicted and actual labels with the sum of squared differences between the actual label values and the mean of actual label values, like this:

R2 = 1- ∑(y-ŷ)2 ÷ ∑(y-ȳ)2

-> Don't worry too much if that looks complicated; most machine learning tools can calculate the metric for you. The important point is that the result is a value between 0 and 1 that describes the proportion of variance explained by the model. 
-> In simple terms, the closer to 1 this value is, the better the model is fitting the validation data. In the case of the ice cream regression model, the R2 calculated from the validation data is 0.95.


d) Iterative training
->The metrics described above are commonly used to evaluate a regression model. 
-> In most real-world scenarios, a data scientist will use an iterative process to repeatedly train and evaluate a model, varying:

d.1) Feature selection and preparation (choosing which features to include in the model, and calculations applied to them to help ensure a better fit).
d.2) Algorithm selection (We explored linear regression in the previous example, but there are many other regression algorithms)
d.3) Algorithm parameters (numeric settings to control algorithm behavior, more accurately called hyperparameters to differentiate them from the x and y parameters).
d.4) After multiple iterations, the model that results in the best evaluation metric that's acceptable for the specific scenario is selected.


16) Binary classification

 https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/5-binary-classification

a) Classification, like regression, is a supervised machine learning technique; and therefore follows the same iterative process of training, validating, and evaluating models. 

a.1) Instead of calculating numeric values like a regression model, the algorithms used to train classification models calculate PROBABILITY VALUES for CLASS ASSIGNMENT and the evaluation metrics used to assess model performance compare the predicted classes to the actual classes.


a.2) Binary classification algorithms are used to train a model that predicts one of two possible labels for a single class. 
-> Essentially, predicting true or false. 
-> In most real scenarios, the data observations used to train and validate the model consist of multiple feature (x) values and a y value that is either 1 or 0.

b) Example - binary classification
-> To understand how binary classification works, let's look at a simplified example that uses a single feature (x) to predict whether the label y is 1 or 0. In this example, we'll use the blood glucose level of a patient to predict whether or not the patient has diabetes. Here's the data with which we'll train the model:

Blood glucose (x)	Diabetic? (y)
67	                    0
103	                    1
114	                    1
72	                    0
116	                    1
65	                    0


c) Training a binary classification model
-> To train the model, we'll use an algorithm to fit the training data to a function that calculates the probability of the class label being true (in other words, that the patient has diabetes). 
-> Probability is measured as a value between 0.0 and 1.0, such that the total probability for all possible classes is 1.0. 
-> So for example, if the probability of a patient having diabetes is 0.7, then there's a corresponding probability of 0.3 that the patient isn't diabetic.

-> There are many algorithms that can be used for binary classification, such as logistic regression, which derives a sigmoid (S-shaped) function with values between 0.0 and 1.0, like this:


c.1) Note

Despite its name, in machine learning logistic regression is used for classification, not regression. The important point is the logistic nature of the function it produces, which describes an S-shaped curve between a lower and upper value (0.0 and 1.0 when used for binary classification).


d) Evaluating a binary classification model
As with regression, when training a binary classification model you hold back a random subset of data with which to validate the trained model. Let's assume we held back the following data to validate our diabetes classifier:


Blood glucose (x)	Diabetic? (y)
66	                    0
107	                    1
112	                    1
71	                    0
87	                    1
89	                    1


d.1) Based on whether the probability calculated by the function is above or below the threshold, the model generates a predicted label of 1 or 0 for each observation. We can then compare the predicted class labels (ŷ) to the actual class labels (y), as shown here:


Blood glucose (x)	Actual diabetes diagnosis (y)	Predicted diabetes diagnosis (ŷ)
66	                           0	                         0
107	                           1	                         1
112	                           1	                         1
71	                           0	                         0
87	                           1	                         0
89	                           1	                         1



d.2) The first step in calculating evaluation metrics for a binary classification model is usually to create a matrix of the number of correct and incorrect predictions for each possible class label:
     y^
    0        1
	---------------
Y 0| 2+		 0
  
  1| 1       3+
   |---------------
   
d.3) This visualization is called a confusion matrix, and it shows the prediction totals where:

			ŷ=0 and y=0: True negatives (TN)
			ŷ=1 and y=0: False positives (FP)
			ŷ=0 and y=1: False negatives (FN)
			ŷ=1 and y=1: True positives (TP)
			
d.4) The arrangement of the confusion matrix is such that correct (true) predictions are shown in a diagonal line from top-left to bottom-right. 
-> Often, color-intensity is used to indicate the number of predictions in each cell, so a quick glance at a model that predicts well should reveal a deeply shaded diagonal trend.



e) Accuracy
-> The simplest metric you can calculate from the confusion matrix is accuracy - the proportion of predictions that the model got right. Accuracy is calculated as:

(TN+TP) ÷ (TN+FN+FP+TP)

-> In the case of our diabetes example, the calculation is:

(2+3) ÷ (2+1+0+3)

= 5 ÷ 6

= 0.83

So for our validation data, the diabetes classification model produced correct predictions 83% of the time.

Accuracy might initially seem like a good metric to evaluate a model, but consider this. Suppose 11% of the population has diabetes. You could create a model that always predicts 0, and it would achieve an accuracy of 89%, even though it makes no real attempt to differentiate between patients by evaluating their features. What we really need is a deeper understanding of how the model performs at predicting 1 for positive cases and 0 for negative cases.

f) Recall
-> Recall is a metric that measures the proportion of positive cases that the model identified correctly. In other words, compared to the number of patients who have diabetes, how many did the model predict to have diabetes?

-> The formula for recall is:

TP ÷ (TP+FN)

-> For our diabetes example:

			3 ÷ (3+1)

			= 3 ÷ 4

			= 0.75

-> So our model correctly identified 75% of patients who have diabetes as having diabetes.

g) Precision
Precision is a similar metric to recall, but measures the proportion of predicted positive cases where the true label is actually positive. In other words, what proportion of the patients predicted by the model to have diabetes actually have diabetes?

The formula for precision is:

TP ÷ (TP+FP)

For our diabetes example:

3 ÷ (3+0)

= 3 ÷ 3

= 1.0

So 100% of the patients predicted by our model to have diabetes do in fact have diabetes.

h) F1-score
-> This score is also used to calculate model's accuracy. 
F1-score is an overall metric that combines recall and precision. The formula for F1-score is:

(2 x Precision x Recall) ÷ (Precision + Recall)

-> For our diabetes example:

(2 x 1.0 x 0.75) ÷ (1.0 + 0.75)

= 1.5 ÷ 1.75

= 0.86


i) AREA UNDER THE CURVE (AUC)
-> Another name for recall is the true positive rate (TPR), and there's an equivalent metric called the false positive rate (FPR) that is calculated as FP÷(FP+TN). 
-> We already know that the TPR for our model when using a threshold of 0.5 is 0.75, and we can use the formula for FPR to calculate a value of 0÷2 = 0.

i.1) Of course, if we were to change the threshold above which the model predicts true (1), it would affect the number of positive and negative predictions; and therefore change the TPR and FPR metrics. 
i.2) These metrics are often used to evaluate a model by plotting a received operator characteristic (ROC) curve that compares the TPR and FPR for every possible threshold value between 0.0 and 1.0:

Diagram of a ROC curve.

i.3) The ROC curve for a perfect model would go straight up the TPR axis on the left and then across the FPR axis at the top. i.4)  Since the plot area for the curve measures 1x1, the area under this perfect curve would be 1.0 (meaning that the model is correct 100% of the time). 
i.5) In contrast, a diagonal line from the bottom-left to the top-right represents the results that would be achieved by randomly guessing a binary label; producing an area under the curve of 0.5. In other words, given two possible class labels, you could reasonably expect to guess correctly 50% of the time.

i.6) In the case of our diabetes model, the curve above is produced, and the area under the curve (AUC) metric is 0.875. 
i.7) Since the AUC is higher than 0.5, we can conclude the model performs better at predicting whether or not a patient has diabetes than randomly guessing.

https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/6-multiclass-classification
17) Multiclass classification:
-> Multiclass classification is used to predict to which of multiple possible classes an observation belongs. 

a) As a supervised machine learning technique, it follows the same iterative train, validate, and evaluate process as regression and binary classification in which a subset of the training data is held back to validate the trained model.

b) Example - multiclass classification
Multiclass classification algorithms are used to calculate probability values for multiple class labels, enabling a model to predict the most probable class for a given observation.

-> Let's explore an example in which we have some observations of penguins, in which the flipper length (x) of each penguin is recorded. For each observation, the data includes the penguin species (y), which is encoded as follows:

0: Adelie
1: Gentoo
2: Chinstrap

-> As with previous examples in this module, a real scenario would include multiple feature (x) values. We'll use a single feature to keep things simple.

Flipper length (x)	Species (y)
167	                   0
172	                   0
225	                   2
197	                   1
189	                   1
232	                   2
158	                   0

https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/6-multiclass-classification
18) Training a multiclass classification model
a) To train a multiclass classification model, we need to use an algorithm to fit the training data to a function that calculates a probability value for each possible class. There are two kinds of algorithm you can use to do this:

One-vs-Rest (OvR) algorithms
Multinomial algorithms


b) One-vs-Rest (OvR) algorithms:

-> One-vs-Rest algorithms train a binary classification function for each class, each calculating the probability that the observation is an example of the target class.
-> Each function calculates the probability of the observation being a specific class compared to any other class. For our penguin species classification model, the algorithm would essentially create three binary classification functions:

f0(x) = P(y=0 | x)
f1(x) = P(y=1 | x)
f2(x) = P(y=2 | x)
-> Each algorithm produces a sigmoid function that calculates a probability value between 0.0 and 1.0. 
-> A model trained using this kind of algorithm predicts the class for the function that produces the highest probability output.


c) Multinomial algorithms
-> As an alternative approach is to use a multinomial algorithm, which creates a single function that returns a multi-valued output. 
-> The output is a vector (an array of values) that contains the probability distribution for all possible classes - with a probability score for each class which when totaled add up to 1.0:

f(x) =[P(y=0|x), P(y=1|x), P(y=2|x)]

-> An example of this kind of function is a softmax function, which could produce an output like the following example:

[0.2, 0.3, 0.5]

-> The elements in the vector represent the probabilities for classes 0, 1, and 2 respectively; so in this case, the class with the highest probability is 2.

-> Regardless of which type of algorithm is used, the model uses the resulting function to determine the most probable class for a given set of features (x) and predicts the corresponding class label (y).


d) Evaluating a multiclass classification model
-> You can evaluate a multiclass classifier by calculating binary classification metrics for each individual class. Alternatively, you can calculate aggregate metrics that take all classes into account.

Let's assume that we've validated our multiclass classifier, and obtained the following results:


Flipper length (x)	Actual species (y)	Predicted species (ŷ)
165						0						0
171						0						0
205						2						1
195						1						1
183						1						1
221						2						2
214						2						2


d.1) Confusion matrix: 

     y^
    0        1		2
	-------------------
Y 0| 2+		 0      0
  
  1| 1       2+		0
  
  2| 0		 1      2+
   |--------------------
   
   
d.2) From this confusion matrix, we can determine the metrics for each individual class as follows:


Class	TP	TN	FP	FN	Accuracy	Recall	Precision	F1-Score
0	   2	5	0	0	1.0	         1.0	1.0				1.0
1	   2	4	1	0	0.86		 1.0	0.67			0.8
2	   2	4	0	1	0.86	     0.67	1.0				0.8


d.3) To calculate the overall accuracy, recall, and precision metrics, you use the total of the TP, TN, FP, and FN metrics:

Overall accuracy = (13+6)÷(13+6+1+1) = 0.90
Overall recall = 6÷(6+1) = 0.86
Overall precision = 6÷(6+1) = 0.86
The overall F1-score is calculated using the overall recall and precision metrics:

d.4) Overall F1-score = (2x0.86x0.86)÷(0.86+0.86) = 0.86

https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/6-multiclass-classification end 



19) Clustering 

https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/7-clustering

a) Clustering is a form of unsupervised machine learning in which observations are grouped into clusters based on similarities in their data values, or features. This kind of machine learning is considered unsupervised because IT DOESN'T MAKE USE OF PREVIOUSLY KNOWN LABEL VALUES TO TRAIN A MODEL. IN A CLUSTERING MODEL, THE LABEL IS THE CLUSTER TO WHICH THE OBSERVATION IS ASSIGNED, BASED ONLY ON ITS FEATURES.

b) Example - clustering
-> For example, suppose a botanist observes a sample of flowers and records the number of leaves and petals on each flower:

Diagram of some flowers.

-> There are no known labels in the dataset, just two features. The goal is not to identify the different types (species) of flower; just to group similar flowers together based on the number of leaves and petals.


Leaves (x1)	Petals (x2)
0				5
0				6
1				3
1				3
1				6
1				8
2				3
2				7
2				8


c) Training a clustering model
There are multiple algorithms you can use for clustering. One of the most commonly used algorithms is K-MEANS CLUSTERING, which consists of the following steps:

1) The feature (x) values are vectorized to define n-dimensional coordinates (where n is the number of features). In the flower example, we have two features: number of leaves (x1) and number of petals (x2). So, the feature vector has two coordinates that we can use to conceptually plot the data points in two-dimensional space ([x1,x2])
2) You decide how many clusters you want to use to group the flowers - call this value k. For example, to create three clusters, you would use a k value of 3. Then k points are plotted at random coordinates. These points become the center points for each cluster, so they're called centroids.
3) Each data point (in this case a flower) is assigned to its nearest centroid.
4) Each centroid is moved to the center of the data points assigned to it based on the mean distance between the points.
5) After the centroid is moved, the data points may now be closer to a different centroid, so the data points are reassigned to clusters based on the new closest centroid.
6) The centroid movement and cluster reallocation steps are repeated until the clusters become stable or a predetermined maximum number of iterations is reached.
The following animation shows this process:




d) Evaluating a clustering model
d.1) Since there's no known label with which to compare the predicted cluster assignments, evaluation of a clustering model is based on how well the resulting clusters are separated from one another.

d.2) There are multiple metrics that you can use to evaluate cluster separation, including:

AVERAGE DISTANCE TO CLUSTER CENTER: How close, on average, each point in the cluster is to the centroid of the cluster.
AVERAGE DISTANCE TO OTHER CENTER: How close, on average, each point in the cluster is to the centroid of all other clusters.
MAXIMUM DISTANCE TO CLUSTER CENTER: The furthest distance between a point in the cluster and its centroid.
Silhouette: A value between -1 and 1 that summarizes the ratio of distance between points in the same cluster and points in different clusters (The closer to 1, the better the cluster separation).

20) Deep learning

a) Deep learning is an advanced form of machine learning that tries to emulate the way the human brain learns. The key to deep learning is the creation of an artificial neural network that simulates electrochemical activity in biological neurons by using mathematical functions, as shown here.

-> Neurons fire in response to electrochemical stimuli. When fired, the signal is passed to connected neurons.	
-> Each neuron is a function that operates on an input value (x) and a weight (w). The function is wrapped in an activation function that determines whether to pass the output on.


b) Artificial neural networks are made up of multiple layers of neurons - essentially defining a deeply nested function. 
-> This architecture is the reason the technique is referred to as deep learning and the models produced by it are often referred to as deep neural networks (DNNs). 

b.1) You can use deep neural networks for many kinds of machine learning problem, including regression and classification, as well as more specialized models for natural language processing and computer vision.

b.2) Just like other machine learning techniques discussed in this module, deep learning involves fitting training data to a function that can predict a label (y) based on the value of one or more features (x). 
b.3) The function (f(x)) is the outer layer of a nested function in which each layer of the neural network encapsulates functions that operate on x and the weight (w) values associated with them. 
b.4) 

The algorithm used to train the model involves iteratively feeding the feature values (x) in the training data forward through the layers to calculate output values for ŷ, validating the model to evaluate how far off the calculated ŷ values are from the known y values (which quantifies the level of error, or loss, in the model), and then modifying the weights (w) to reduce the loss. The trained model includes the final weight values that result in the most accurate predictions.


21) Example - Using deep learning for classification
-> To better understand how a deep neural network model works, let's explore an example in which a neural network is used to define a classification model for penguin species.

Diagram of a neural network used to classify a penguin species.

a) The feature data (x) consists of some measurements of a penguin. Specifically, the measurements are:

The length of the penguin's bill.
The depth of the penguin's bill.
The length of the penguin's flippers.
The penguin's weight.
In this case, x is a vector of four values, or mathematically, x=[x1,x2,x3,x4].

b) The label we're trying to predict (y) is the species of the penguin, and that there are three possible species it could be:

Adelie
Gentoo
Chinstrap
c) This is an example of a classification problem, in which the machine learning model must predict the most probable class to which an observation belongs. A classification model accomplishes this by predicting a label that consists of the probability for each class. In other words, y is a vector of three probability values; one for each of the possible classes: [P(y=0|x), P(y=1|x), P(y=2|x)].

The process for inferencing a predicted penguin class using this network is:

c.1) The feature vector for a penguin observation is fed into the input layer of the neural network, which consists of a neuron for each x value. In this example, the following x vector is used as the input: [37.3, 16.8, 19.2, 30.0]
c.2) The functions for the first layer of neurons each calculate a weighted sum by combining the x value and w weight, and pass it to an activation function that determines if it meets the threshold to be passed on to the next layer.
c.3) Each neuron in a layer is connected to all of the neurons in the next layer (an architecture sometimes called a fully connected network) so the results of each layer are fed forward through the network until they reach the output layer.
c.4) The output layer produces a vector of values; in this case, using a softmax or similar function to calculate the probability distribution for the three possible classes of penguin. In this example, the output vector is: [0.2, 0.7, 0.1]
c.5) The elements of the vector represent the probabilities for classes 0, 1, and 2. The second value is the highest, so the model predicts that the species of the penguin is 1 (Gentoo).


22) How does a neural network learn?

-> The weights in a neural network are central to how it calculates predicted values for labels. During the training process, the model learns the weights that will result in the most accurate predictions. Let's explore the training process in a little more detail to understand how this learning takes place.

Diagram of a neural network being trained, evaluated, and optimized.

a) The training and validation datasets are defined, and the training features are fed into the input layer.
b) The neurons in each layer of the network apply their weights (which are initially assigned randomly) and feed the data through the network.
c) The output layer produces a vector containing the calculated values for ŷ. For example, an output for a penguin class prediction might be [0.3. 0.1. 0.6].
d) A loss function is used to compare the predicted ŷ values to the known y values and aggregate the difference (which is known as the loss). For example, if the known class for the case that returned the output in the previous step is Chinstrap, then the y value should be [0.0, 0.0, 1.0]. The absolute difference between this and the ŷ vector is [0.3, 0.1, 0.4]. In reality, the loss function calculates the aggregate variance for multiple cases and summarizes it as a single loss value.
e) Since the entire network is essentially one large nested function, an optimization function can use differential calculus to evaluate the influence of each weight in the network on the loss, and determine how they could be adjusted (up or down) to reduce the amount of overall loss. The specific optimization technique can vary, but usually involves a gradient descent approach in which each weight is increased or decreased to minimize the loss.
f) The changes to the weights are backpropagated to the layers in the network, replacing the previously used values.
g) The process is repeated over multiple iterations (known as epochs) until the loss is minimized and the model predicts acceptably accurately.
 Note
 
 
 
23) Azure Machine Learning
-> Microsoft Azure Machine Learning is a cloud service for training, deploying, and managing machine learning models. 
-> It's designed to be used by data scientists, software engineers, devops professionals, and others to manage the end-to-end lifecycle of machine learning projects, including:

a) Exploring data and preparing it for modeling.
b) Training and evaluating machine learning models.
c) Registering and managing trained models.
d) Deploying trained models for use by applications and services.
e) Reviewing and applying responsible AI principles and practices.



https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/9-azure-machine-learning


24) Features and capabilities of Azure Machine Learning
-> Azure Machine Learning provides the following features and capabilities to support machine learning workloads:

a) Centralized storage and management of datasets for model training and evaluation.
b) On-demand compute resources on which you can run machine learning jobs, such as training a model.
c) Automated machine learning (AutoML), which makes it easy to run multiple training jobs with different algorithms and parameters to find the best model for your data.
d) Visual tools to define orchestrated pipelines for processes such as model training or inferencing.
e) Integration with common machine learning frameworks such as MLflow, which make it easier to manage model training, evaluation, and deployment at scale.
f) Built-in support for visualizing and evaluating metrics for responsible AI, including model explainability, fairness assessment, and others.


25) Provisioning Azure Machine Learning resources
-> The primary resource required for Azure Machine Learning is an Azure Machine Learning workspace, which you can provision in an Azure subscription. Other supporting resources, including storage accounts, container registries, virtual machines, and others are created automatically as needed.

-> To create an Azure Machine Learning workspace, you can use the Azure portal, as shown here:


Azure portal home -> Create a resource -> Marketplace 



26) Azure Machine Learning studio
-> After you've provisioned an Azure Machine Learning workspace, you can use it in Azure Machine Learning studio; 
-> a browser-based portal for managing your machine learning resources and jobs.

In Azure Machine Learning studio, you can (among other things):

● Import and explore data.
● Create and use compute resources.
● Run code in notebooks.
● Use visual tools to create jobs and pipelines.
● Use automated machine learning to train models.
● View details of trained models, including evaluation metrics, responsible AI information, and training parameters.
● Deploy trained models for on-request and batch inferencing.
● Import and manage models from a comprehensive model catalog.

-> The screenshot shows the Metrics page for a trained model in Azure Machine Learning studio, in which you can see the evaluation metrics for a trained multiclass classification model.



27) Explore Automated Machine Learning in Azure Machine Learning

-> In this exercise, you’ll use the automated machine learning feature in Azure Machine Learning to train and evaluate a machine learning model. 
-> You’ll then deploy and test the trained model.

This exercise should take approximately 30 minutes to complete.

a) CREATE AN AZURE MACHINE LEARNING WORKSPACE
-> To use Azure Machine Learning, you need to provision an AZURE MACHINE LEARNING WORKSPACE in your Azure subscription.
->  Then you’ll be able to use AZURE MACHINE LEARNING STUDIO to work with the resources in your workspace.

a.1) Sign into the Azure portal at https://portal.azure.com using your Microsoft credentials.

a.2) Select + Create a resource, search for Machine Learning, and create a new Azure Machine Learning resource with the following settings:
► Subscription: Your Azure subscription.
► Resource group: Create or select a resource group.
► Name: Enter a unique name for your workspace.
► Region: Select the closest geographical region.
► Storage account: Note the default new storage account that will be created for your workspace.
► Key vault: Note the default new key vault that will be created for your workspace.
► Application insights: Note the default new application insights resource that will be created for your workspace.
► Container registry: None (one will be created automatically the first time you deploy a model to a container).


a.3) Select Review + create, then select Create. Wait for your workspace to be created (it can take a few minutes), and then go to the deployed resource.

a.4) Select Launch studio (or open a new browser tab and navigate to https://ml.azure.com, and sign into Azure Machine Learning studio using your Microsoft account). Close any messages that are displayed.

a.5)In Azure Machine Learning studio, you should see your newly created workspace. If not, select All workspaces in the left-hand menu and then select the workspace you just created.


b) USE AUTOMATED MACHINE LEARNING TO TRAIN A MODEL

-> Automated machine learning enables you to try multiple algorithms and parameters to train multiple models, and identify the best one for your data. 
-> In this exercise, you’ll use a dataset of historical bicycle rental details to train a model that predicts the number of bicycle rentals that should be expected on a given day, based on seasonal and meteorological features.


b.1) bIn Azure Machine Learning studio, view the Automated ML page (under Authoring).

b.2) Create a new Automated ML job with the following settings, using Next as required to progress through the user interface:

b.3) Basic settings:

□ Job name: mslearn-bike-automl
□ New experiment name: mslearn-bike-rental
□ Description: Automated machine learning for bike rental prediction
□ Tags: none


b.4) Task type & data:

□ Select task type: Regression
□ Select dataset: Create a new dataset with the following settings:
	□ Data type:
		Name: bike-rentals
		Description: Historic bike rental data
		Type: Table (mltable)
		
	□ Data source:
		Select From local files
	□ Destination storage type:
		Datastore type: Azure Blob Storage
		Name: workspaceblobstore
	□ MLtable selection:
		Upload folder: Download and unzip the folder that contains the two files you need to upload https://aka.ms/bike-rentals
-> Select Create. After the dataset is created, 
-> select the bike-rentals dataset to continue to submit the Automated ML job.


b.5) TASK SETTINGS

□ Task type: Regression
□ Dataset: bike-rentals
□ Target column: rentals (integer)
=> Additional configuration settings:
	□ Primary metric: NormalizedRootMeanSquaredError
	□ Explain best model: Unselected
	□ Enable ensemble stacking: Unselected
	□ Use all supported models: Unselected.
	You’ll restrict the job to try only a few specific algorithms.
	□ Allowed models: Select only 
	RandomForest and LightGBM — normally you’d want to try as many as possible, but each model added increases the time it takes to run the job.
	
	
Limits: Expand this section
